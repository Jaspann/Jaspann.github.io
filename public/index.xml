<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home Page on William Parker&#39;s Personal Site</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home Page on William Parker&#39;s Personal Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>wiparker2020@gmail.com (William Parker)</managingEditor>
    <webMaster>wiparker2020@gmail.com (William Parker)</webMaster>
    <copyright>William Parker</copyright>
    <lastBuildDate>Sun, 09 Feb 2025 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Chord Llama</title>
      <link>http://localhost:1313/post/ai-projects/chord-llama/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/ai-projects/chord-llama/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;Fine-tuning Llama for sheet music generation.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Large Language Models (LLMs) have recently reached a critical mass in the public eye,
where everyone is trying to apply the technology to any problem they can think of.
They have been further trained, or fine-tuned, for a variety of use cases, be it for
business, software development, or speaking languages outside their original training
data. At the time, I had recently learned about &lt;a href=&#34;https://www.w3.org/2021/06/musicxml40/&#34;&gt;MusicXML&lt;/a&gt;
which is the format used for sheet music. An application like &lt;a href=&#34;https://musescore.org/en&#34;&gt;MuseScore&lt;/a&gt;
is used to write music sheets for musicians and saves the work to a &lt;code&gt;.MXL&lt;/code&gt; file so it can be
redistributed and edited without the pains of using a proprietary file type or making
the sheets by hand and scanning them with a printer. MusicXML has been a standard since 2004
and managed by W3C by 2015. As it is a pretty widely accepted standard, we looked into
understanding how an LLM like Llama could be fine-tuned on this data to continue existing music
or generate its own.&lt;/p&gt;
&lt;p&gt;Music generation in the AI world is actually &lt;a href=&#34;https://arxiv.org/abs/2210.13944&#34;&gt;not a new concept&lt;/a&gt;.
&lt;a href=&#34;https://arxiv.org/pdf/2210.10349&#34;&gt;MuseFormer&lt;/a&gt; was released by Microsoft in 2022 and applies
the transformer architecture to music. In their work, each concept like tempo, beat,
pitch, and duration is a separate token. The attention mechanism has two stages the fine-grained
attention only attends to the elements in the same bar, while the coarse-grained attention
only attends to the summary of other bars. This allows it to be aware of the entire song without
saving all of it in the context length.&lt;/p&gt;
&lt;p&gt;An issue we saw with models like MuseFormer is that the model required a significant amount of
technical knowledge to set up. This creates a barrier for entry that is non-existent with the
flurry of open-source tools anyone can use to run LLMs on their machine. By instead fine-tuning
Llama as a base of our project, while we would not have the tokenizer or architecture built for
this use case, it would be able to run on any machine, easing the availability to end users
greatly.&lt;/p&gt;
&lt;h2 id=&#34;data-collection&#34;&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;One of the more prevalent issues in AI training has been training on copyrighted material.
We wanted to make sure that we did not fall into a legal grey area with the existence of this model,
and to solve this we were able to get our sources from classical works in the public domain. Our first source was
&lt;a href=&#34;http://mscorelib.com/&#34;&gt;MScoreLib&lt;/a&gt; which was a project that scanned classical works into MusicXML
using automated tooling. The second source was
&lt;a href=&#34;http://www.synthzone.com/forum/ubbthreads.php/topics/384909&#34;&gt;Wikifonia&lt;/a&gt; which was an
online encyclopedia for public domain sheet music.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning--preparation&#34;&gt;Data Cleaning &amp;amp; Preparation&lt;/h2&gt;
&lt;p&gt;Cleaning the data to get it into a usable form for Llama is what took up the majority of
my time in this project.&lt;/p&gt;
&lt;p&gt;The first thing I saw as a problem was the context size of Llama 3. Unlike 3.1&amp;rsquo;s 128k context size,
Llama 3 had only 4k tokens to use. Many special characters that XML uses take up their own token.
XML, and MusicXML especially, are very verbose standards, making it hard to fit much within the context
size. The first thing I wanted to do was remove the lyrics and all the unnecessary tags in the file
that did not affect the end appearance of the sheet music or were too rare to be learned by the model.
This required going over the full MusicXML specification and creating lists for all of the elements and
attributes that should either be removed or kept in the file. MusicXML allows to adjust the size and
color of text, making pixel-level adjustments on the location of the note, and many more attributes
that do not affect the appearance or sound of the file when played.&lt;/p&gt;
&lt;p&gt;Removing all unnecessary elements was still not enough for most songs to fit within the 4k context length.
To further condense the music, we decided to convert the XML to YAML format. We decided on YAML as it
has been shown to be &lt;a href=&#34;https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df&#34;&gt;more token efficient and boost the performance of models&lt;/a&gt;
Doing this required us to index each musical part with &lt;code&gt;a00_&lt;/code&gt; where a was necessary because the program
could not handle YAML starting with a number and &lt;code&gt;00&lt;/code&gt; was the number of the musical part. YAML is
commonly used for config files and is not intended for this use case at all. In practice though, this was
exactly what we needed to get the format small enough to pass in multiple staves to train the model on.&lt;/p&gt;
&lt;p&gt;To visualize how much we condensed the format, on the left is an example of three notes of MusicXML,
and on the right are the same three notes after our cleaning that we would send to the model.&lt;/p&gt;



&lt;div class=&#34;gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition&#34; itemscope itemtype=&#34;http://schema.org/ImageGallery&#34;&gt;
    
  
  &lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/css/hugo-easy-gallery.css&#34; /&gt;
  &lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34;
    itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
      &lt;div class=&#34;img&#34; style=&#34;background-image: url(&#39;/ai-projects/chord-llama/musicXML.png&#39;);&#34;&gt;
        &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/ai-projects/chord-llama/musicXML.png&#34; /&gt;
      &lt;/div&gt;
      &lt;a href=&#34;http://localhost:1313/ai-projects/chord-llama/musicXML.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
    &lt;/figure&gt;
  &lt;/div&gt;

  
  
  &lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34;
    itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
      &lt;div class=&#34;img&#34; style=&#34;background-image: url(&#39;/ai-projects/chord-llama/yaml.png&#39;);&#34;&gt;
        &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/ai-projects/chord-llama/yaml.png&#34; /&gt;
      &lt;/div&gt;
      &lt;a href=&#34;http://localhost:1313/ai-projects/chord-llama/yaml.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
    &lt;/figure&gt;
  &lt;/div&gt;


&lt;/div&gt;

&lt;p&gt;Additionally, we needed to be able to undo the YAML conversion so that we can verify that our steps worked
and once the model is outputting new data in this format we can turn it into MusicXML to be viewed and listened to.&lt;/p&gt;
&lt;p&gt;Finally, we needed to split the songs to fit inside the 4k token limit. We split each song into even chunks
of up to 3500 tokens and put half of each in the input and output columns. We put the data in
&lt;a href=&#34;https://jsonlines.org/&#34;&gt;JSON Lines&lt;/a&gt; to be used for training and uploaded to HuggingFace.&lt;/p&gt;
&lt;p&gt;The cleaned data in our format is available on HuggingFace along with the notebook used to create it:
&lt;a href=&#34;https://huggingface.co/datasets/Chord-Llama/chord_llama_dataset&#34;&gt;https://huggingface.co/datasets/Chord-Llama/chord_llama_dataset&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;fine-tuning&#34;&gt;Fine-Tuning&lt;/h2&gt;
&lt;p&gt;To train, the technical process we used was Low-Rank Adaptation (LoRA). LoRA works by adding
small, adjustable components to each layer of the model, leaving the original model unchanged.
Instead of using a single delta matrix, LoRA employs two matrices to reduce the dimensionality.
Because there are fewer calculations and memory needed, the fine-tuning process is much faster.
Additionally, this method also typically produces higher-quality results than normal fine-tuning.&lt;/p&gt;
&lt;p&gt;We used &lt;a href=&#34;https://unsloth.ai/&#34;&gt;Unsloth&lt;/a&gt; to help us fine-tune with LoRA. Unsloth allowed us to easily
configure many parameters in training, as well as connect with &lt;a href=&#34;https://wandb.ai/site/&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;
to record all of our tests. We were able to optimize scheduling, learning rate, weight decay, batch size, and
gradient accumulation steps.&lt;/p&gt;
&lt;p&gt;Our final dataset had almost 100k rows of training data, which we knew was more than enough for our purposes.
To start, we trained on 10k samples but quickly found that this much training overfit the model, and it
would be better to pick out a smaller sample size. The model did not improve after 1k samples, so we ended
up training with 1.6k samples.&lt;/p&gt;
&lt;h2 id=&#34;optimization-and-publishing&#34;&gt;Optimization and Publishing&lt;/h2&gt;
&lt;p&gt;Just because a model is fine-tuned it does not mean that it is able to be used. After training, the model
is split across multiple &lt;code&gt;.safetensor&lt;/code&gt; files. While this is useful for further development, for creating an
end-user application, it needs to be in a format that optimizes for inference. This often means converting
the checkpoint into &lt;a href=&#34;https://github.com/ggml-org/ggml/tree/master&#34;&gt;GGUF&lt;/a&gt;, which is a standard .bin file that is
optimized for quick loading and saving and is standard for locally hosted inference applications. This was done
using &lt;a href=&#34;https://github.com/ggml-org/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;. Quantizing the model to four-bit precision allowed it to
be run on the laptop for presenting the work, so we did that as well. I was able to publish this model on
&lt;a href=&#34;https://ollama.com/jaspann/llama-3-chord-llama-2&#34;&gt;Ollama&lt;/a&gt; for anyone to use.&lt;/p&gt;
&lt;p&gt;The final component of the project was creating a small web application to show the model working. I decided
that I wanted to use Flask for the backend and React for the front end, both new to me. While it was not the prettiest,
it showed that from the data I put in, it was able to generate the music in real-time for the audience to see.
It was unfortunately limited to the YAML, rather than converting back into MusicXML, but the result was more than enough
to prove the functionality of the project.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  class=&#34;bordered-image&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/ai-projects/chord-llama/chord_llama_website.png&#34; alt=&#34;The Chord Llama website.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/ai-projects/chord-llama/chord_llama_website.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;The Chord Llama website.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I loved the idea and our work in Chord Llama. I feel that with some more work and modern LLM technology, we could have
had an LLM capable of generating good music that solved the goals we set for this project. Working with the unique problem
of fitting MusicXML within a limited token context is exactly the fun and creative type of solution I love working on in software development.&lt;/p&gt;
&lt;p&gt;Here are the links to the project. In addition, this project was part of my time at San Jose State University, and my partner
and I wrote up a report at the end of the semester that you can read below for more information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Web Project on GitHub: &lt;a href=&#34;https://github.com/Chord-Llama/Chord-Llama&#34;&gt;https://github.com/Chord-Llama/Chord-Llama&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HuggingFace: &lt;a href=&#34;https://huggingface.co/Chord-Llama&#34;&gt;https://huggingface.co/Chord-Llama&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;object
    data=&#34;/ai-projects/chord-llama/Chord_Llama_Final.pdf&#34; width=&#34;100%&#34; height=&#34;800px&#34;type=&#34;application/pdf&#34;
&gt;
    &lt;p&gt;Unable to display PDF file. &lt;a href=&#34;http://localhost:1313/ai-projects/chord-llama/Chord_Llama_Final.pdf&#34;&gt;Download&lt;/a&gt; instead.&lt;/p&gt;
&lt;/object&gt;
</description>
      
    </item>
    
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/about/</guid>
      
        <description>&lt;p&gt;The intention of this site is to provide a place where I can record the past projects I worked on.
It&amp;rsquo;s meant to be somewhere between a portfolio and a retrospective blog, where I discuss what I
worked on from a technical perspective and show off the design process behind the features that were added.&lt;/p&gt;
&lt;p&gt;The date of the post the approximate date I finished the project, not the date of writing.&lt;/p&gt;
&lt;p&gt;Here is a list of all of the posts I have planned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Moore&amp;rsquo;s Falls Eagle Scout Project&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; APCSA TA&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SNHU Computer Science Club VP&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;FreeMoveVR&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SJSU MS Thesis&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Unified Live Chat&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SJSU MS AI Education&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; SNHU Comp Sci Education&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; 3tene AHK&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Eventified&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; FlightSure&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; ChordLlama&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Multi-Repo Coding Assistant&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Pond Jumper&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; RoundHouse&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Yabi&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Flutter M BLE&lt;/li&gt;
&lt;li&gt;&lt;input checked=&#34;&#34; disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; Google Mediapipe Pose Detection&lt;/li&gt;
&lt;li&gt;&lt;input disabled=&#34;&#34; type=&#34;checkbox&#34;&gt; &lt;strong&gt;Arity Android Engineer Intern&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
    <item>
      <title>Multi Repo Coding Assistant</title>
      <link>http://localhost:1313/post/ai-projects/multi-repo-coding-assistant/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/ai-projects/multi-repo-coding-assistant/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;Applying LLM routing for a single chat across multiple repositories.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Many of the projects I&amp;rsquo;ve worked on combine multiple repositories to create a single experience
for the user. There are many common examples of this, like a full-stack developer working on
the front and back end. It gets much more complex though as the scope of the application grows. Companies
may want development teams for Android and iOS, and this requires teams to understand where each
other team is at to maintain feature parity. There are many questions that a developer of one team
could have, and discussion on every technical intricacy can take time from development.&lt;/p&gt;
&lt;p&gt;Coding virtual assistants have become increasingly common over the last few years.
Virtual assistants can help to better understand whatever question you ask on the topic it is
made for. In another project of mine, I have been working on a suite of products that spanned across
different platforms and coding languages. While most assistants only respond to questions about a single
repository, my project would not work with most existing off-the-shelf solutions as I need to
constantly think about how all three repositories interact at all times.&lt;/p&gt;
&lt;p&gt;Creating a RAG system that most tools can generate automatically in this case would be difficult
for multiple reasons. Firstly, a naïve implementation of throwing all of my code into a single system
will create a situation where it may find the most relevant files to be from the wrong project. Secondly,
I would likely need to parse the files to extract the doc comments, function headers, and code body
to minimize confusing the model.&lt;/p&gt;
&lt;h2 id=&#34;cleaning-the-data&#34;&gt;Cleaning the Data&lt;/h2&gt;
&lt;p&gt;The project I wanted to build this virtual assistant (VA) for had its code base split across three repositories.
There was a Python desktop app, a Flutter mobile app, and a C++ project. I needed to get all of these code
files to the point I would be happy to pass them into my RAG.&lt;/p&gt;
&lt;p&gt;I wanted to split each function into its doc comment, header, and body components, which I would find would be
extremely difficult for some languages. I wanted to do everything in a Python notebook and looked at libraries to
do this for the Python project as it was likely going to be the easiest of the three. Python has a very easy internal
system to do this in their standard library called &lt;a href=&#34;https://docs.python.org/3/library/ast.html&#34;&gt;ast&lt;/a&gt; (Abstract Syntax Trees).
With &lt;code&gt;ast&lt;/code&gt;, I could &lt;code&gt;walk&lt;/code&gt; each file, and extract the contents of each part of the file as I wanted it,
making this step very easy.&lt;/p&gt;
&lt;p&gt;To solve this problem for Flutter (Dart) and C++, I decided my best option would be complex RegEx expressions.
In most situations, I prefer to code these types of projects myself or get help by working together with an LLM
so I always know what is happening in the code and the LLM only gives hints on where I
am going wrong and correcting me as I go. With something like RegEx though, it is one of those tools
where it is very precise and often feels like a waste of time researching to implement the one problem
you have for your use case. Because of how complex the problem is and the limited scope it had in the
project, I decided to pass in code of the languages for LLMs to generate RegEx expressions and process them
for me.&lt;/p&gt;
&lt;p&gt;While it did not work at first, after a dozen iterations I got the RegEx and code to work with almost
all of the code blocks I was checking. With C++, I had some getter and setter functions that were only
a few lines so I added a check that the code needed to be more than 10 lines long to be saved for RAG.
Flutter was even more complex, likely as it is a newer and less popular language.&lt;/p&gt;
&lt;h2 id=&#34;organizing-and-optimizing-for-rag&#34;&gt;Organizing and Optimizing For RAG&lt;/h2&gt;
&lt;p&gt;Before passing my data into RAG, I needed to figure out the best way to chunk the data.
I would have liked for the entire file to be directly passed in, but the problem was that some files
were too long for the embedder to handle. I needed to split the files, and the common convention is to
simply split them every set number of characters. I was not sure if this was the best approach, as
this could split the data of a function in two, so alongside creating databases in the character
chunk format, I also broke the file into its individual functions, which was small enough for
the database.&lt;/p&gt;
&lt;p&gt;Now that I had my data as I wanted it, I needed to decide how to pass it into the RAG database.
I am a fan of &lt;a href=&#34;https://www.youtube.com/channel/UCHaF9kM2wn8C3CLRwLkC2GQ&#34;&gt;Matt Williams&lt;/a&gt;
on YouTube, and a month before this project he posted a video about
&lt;a href=&#34;https://youtu.be/76EIC_RaDNw&#34;&gt;the best ways to embed&lt;/a&gt;. His channel focuses on
&lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;, which I was going to use for the project,
and he integrated RAG using an embedding model with prefixes called
&lt;a href=&#34;https://arxiv.org/abs/2405.05374&#34;&gt;snowflake-arctic-embed&lt;/a&gt;.
I loosely followed his tutorial and set up my environment with Ollama for
managing the LLMs and &lt;a href=&#34;https://www.trychroma.com/&#34;&gt;Chroma&lt;/a&gt; for vector storage.&lt;/p&gt;
&lt;p&gt;At this point, I had six databases. I had my two chunking methods, multiplied by the
three repositories I was creating the assistant for. I now needed to do the unique part
of my project, creating the routing to know which database to hit when a user asks a question.
A &lt;a href=&#34;https://docs.llamaindex.ai/en/stable/module_guides/querying/router/&#34;&gt;router&lt;/a&gt; in the world of
LLMs is pretty simple. Before it responds to a prompt we add a system instruction to tell it that
instead of answering the question, we want it to respond with a pre-defined set of choices instead.&lt;/p&gt;
&lt;p&gt;We can use this concept to have a single interface for three different RAG databases. We ask
the router to figure out which project the user&amp;rsquo;s question is asking about, and send it to the
correct model. This often uses a smaller model, but for my use case, I was already using small
models so I used the same one as the model that answered the question. I chose to use Meta&amp;rsquo;s
&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Llama 3.1 8b&lt;/a&gt; and
&lt;a href=&#34;https://github.com/ibm-granite/granite-3.0-language-models/blob/main/paper.pdf&#34;&gt;granite3-dense:8b&lt;/a&gt;
in testing. Llama is very well known, and Granite3 was used in the Mat Williams video as a very good
model for RAG applications.&lt;/p&gt;
&lt;p&gt;In the system, I made sure to include a system prompt and prefixes. The system prompt is just what
we as the designers of the application are telling the model what to do. In this case, it is just a simple
prompt to make sure that the routing is done correctly and that it does not deviate when it tries to answer
the question. LLMs are non-deterministic, so we need to also code in safeguards as well. The prefixes we need for
our embedding model are just a simple string we add to the start of the prompt in addition to the system prompt
so that it embeds optimally. For snowflake-arctic-embed, the prefix is &amp;ldquo;Represent this sentence for searching
relevant passages: &amp;ldquo;.&lt;/p&gt;
&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;
&lt;p&gt;I created 20 questions to test the model on, with about seven for each of the three repositories.
For each question, I logged both the response of the router, the results from Chroma and, the final output
so I would know where the issue went wrong. Maybe 70% of the time that there was an error, it was due to
the wrong files being returned by RAG, rather than an issue with the LLM model.&lt;/p&gt;
&lt;p&gt;In the presentation, you can see the benchmarks on the final slide. Llama 3.1 and Granite had very similar scores
for both chunking methods. The character-based chunking was significantly better than my function-based solution.&lt;/p&gt;
&lt;iframe src=&#34;https://1drv.ms/p/c/78ac3e9c395f4290/IQTeSdxR92W3QY7pA3mtUBsAAaSRcWdEMoDNXt72TIjfmxM?em=2&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;100%&#34; height=&#34;400px&#34; frameborder=&#34;0&#34;&gt;This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office&lt;/a&gt;.&lt;/iframe&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This idea is somewhat promising in my opinion. I find much of the LLM product &amp;ldquo;solutions&amp;rdquo; to be hype,
and while this problem does fall into that category, there are many people who are interested and want to use them.
I can see the tool being useful for someone who would want to contribute to my software suite, where scouring
between three repositories in different languages to find the correct way to match compatibility is not feasible
to expect from the average contributor otherwise. Maybe it could be useful for some other niche cases as well, but
any real company structure should have communication or some sort of bureaucracy in place so this is not needed.
Additionally, any real attempt would need tools for every supported language, and I am not willing to put
research into a dozen languages for a product I don&amp;rsquo;t want to use myself.&lt;/p&gt;
&lt;p&gt;I am still happy that I worked on this project and got the results I did. I feel that it is a unique touch to the
existing code assistants that I have not seen much of. If you are interested, the project is available on my GitHub.
Please note, that I made it specifically for another project of mine, FreeMoveVR, which is not public as of writing,
but it should work on any other project with the basic structure:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Jaspann/FreeMoveVR_VA&#34;&gt;https://github.com/Jaspann/FreeMoveVR_VA&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>FlightSure</title>
      <link>http://localhost:1313/post/ai-projects/flight-sure/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/ai-projects/flight-sure/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;Using Machine Learning to Find the Chance a Flight is on Time.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I had recently returned to San Jose after visiting my family across the country.
While waiting at Logan Airport in Boston, I was watching the snowfall and wondered
if it would delay my flight. It had just started and light snow is a normal occurrence for
the area, so it was unlikely but possible. I then considered San Jose&amp;rsquo;s Airport. While they have
different weather they need to account for, there is no infrastructure in place for even
light snow. In addition, many drivers would not know how to handle slippery roads, which
would probably cause significant problems across the city.&lt;/p&gt;
&lt;p&gt;While I can&amp;rsquo;t do much about the weather, I wanted to see if I could put a numerical value to the
chance my flight would be delayed or canceled as an outside observer.&lt;/p&gt;
&lt;h2 id=&#34;verifying-intuition&#34;&gt;Verifying Intuition&lt;/h2&gt;
&lt;p&gt;Flights are often canceled only a few hours before they otherwise would be scheduled to take off.
The airline&amp;rsquo;s app or website is the best you can make plans around, but I
wanted to try to predict what would happen up to a few days in advance.&lt;/p&gt;
&lt;p&gt;The first thought would be that most flight problems, and the easiest to predict, are ones due to weather.
A look at the United States Department of Transportation (US DOT) statistics shows that this assumption has some merit.
While their page on
&lt;a href=&#34;https://www.transtats.bts.gov/ot_delay/OT_DelayCause1.asp?20=E&#34;&gt;Airline On-Time Statistics and Delay Causes&lt;/a&gt;
shows that only 0.22% of all flights are impacted by weather (filter between Jan 2021 and Dec 2023). This statistic fails to account for
&lt;a href=&#34;https://www.transtats.bts.gov/ot_delay/ot_delaycause1.asp?6B2r=I&amp;amp;20=E&#34;&gt;National Aviation System Delays&lt;/a&gt;
which make up the greatest category of flight problems outside of delayed arrivals and &amp;ldquo;air carrier delays&amp;rdquo;.
Looking into National Aviation System Delays further, weather makes up almost half of all delays in this category.
While there are clearly several other factors that can cause flights to be delayed, predicting based on weather
seemed like the best predictor we could realistically use without insider knowledge of internal systems.&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  class=&#34;bordered-image&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/flight-sure/On-Time_Arrival_Performance.png&#34; alt=&#34;Airline On Time Statistics and Delay Causes, Jan 2021 to Dec 2023.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/flight-sure/On-Time_Arrival_Performance.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;Airline On Time Statistics and Delay Causes, Jan 2021 to Dec 2023.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div style=&#34;margin: 2rem 0;&#34;&gt;&lt;/div&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  class=&#34;bordered-image&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/flight-sure/Causes_of_National_Aviation_System_Delays.png&#34; alt=&#34;National Aviation System Delays, Jan 2021 to Dec 2023&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/flight-sure/Causes_of_National_Aviation_System_Delays.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;National Aviation System Delays, Jan 2021 to Dec 2023&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&#34;gathering-data&#34;&gt;Gathering Data&lt;/h2&gt;
&lt;p&gt;The US DOT and National Oceanic and Atmospheric Administration (NOAA) both have public
records of statistics in their respective domains freely accessible to the public. I wanted to build classical
machine learning models for each airport to measure delays and cancellations, so I needed to merge tabular data
based on time and airport to find under what conditions cancellations and delays occur.&lt;/p&gt;
&lt;p&gt;NOAA has a page called
&lt;a href=&#34;https://www.climate.gov/maps-data/dataset/past-weather-zip-code-data-table&#34;&gt;Past Weather by Zip Code - Data Table&lt;/a&gt;
Which was available to the public to access weather reporting.
While this looked like everything I needed, there were problems working with it,
and I ended up using Automated Surface Observing System (ASOS) Network instead. A frontend to the data was available through 
&lt;a href=&#34;https://mesonet.agron.iastate.edu/ASOS/&#34;&gt;Iowa State University&lt;/a&gt;, which had more data and hourly reports from all major US airports,
which was perfect for our use case. Similarly, the US DOT has a page called
&lt;a href=&#34;https://transtats.bts.gov/DL_SelectFields.aspx?gnoyr_VQ=FGJ&amp;amp;QO_fu146_anzr=b0-gvzr&#34;&gt;On-Time : Reporting Carrier On-Time Performance (1987-present)&lt;/a&gt;
which allows the public to get extremely detailed information on exact logs of airline cancellations and delays. I decided to get the past three years
of data for the 30 most popular airports in the country.&lt;/p&gt;
&lt;h2 id=&#34;data-cleaning-and-eda&#34;&gt;Data Cleaning and EDA&lt;/h2&gt;
&lt;p&gt;I needed to clean the data, converting times and locations to exactly match between datasets, and merge the two together.
Interacting with such a large dataset turned out to be unreasonable for the software I was using, so I decided I would split the
data by airport and loop over the datasets separately so I would not encounter any issues with size.&lt;/p&gt;
&lt;p&gt;I then needed to figure out which weather variables most correlated with flight problems so I would not be passing in useless information
to the model. I created a heatmap correlating weather events to delays and cancellations and removed all the variables that looked
like they had no impact on the flight.&lt;/p&gt;
&lt;p&gt;Another issue I found when preparing for modeling was the imbalance of classes. About ten percent of the flights were delayed, with many
only being delayed a few minutes. I decided that I should classify between lengths of time delayed when classifying delays, but I should
have another model that predicts how long it will be delayed as well.&lt;/p&gt;
&lt;p&gt;Studying how models perform under a class imbalance, I used weighted class random forests for classification, and I applied simple linear
regression to predict the length of the delay. I only would attempt to find the length of the delay if there was a greater than 50% chance
that the flight would be delayed according to the model. Due to this, the training data of the model only consisted of the delayed flights.&lt;/p&gt;
&lt;h2 id=&#34;final-application&#34;&gt;Final Application&lt;/h2&gt;
&lt;p&gt;Making my work into an end-user application for people is important to me. When discussing ideas with people, having proof of a model or algorithm
working is often not enough. To make the idea &amp;ldquo;click&amp;rdquo; for people, some type of interface where the user can play around is needed.&lt;/p&gt;
&lt;p&gt;If I am accepting user input, users will not want to input all of the model&amp;rsquo;s variables by hand. Instead, it would be much more reasonable if they
only need to pass in the airport and time of departure, and we can do the rest automatically. To do this, of course, we need access to a weather API.
I found that &lt;a href=&#34;https://openweathermap.org/forecast5&#34;&gt;OpenWeather&amp;rsquo;s 5 Day Forecast&lt;/a&gt; worked extremely well for our use case. It&amp;rsquo;s free and as this was
not meant to be published, I only need a few dozen called for bug testing and showing off the proof of concept. Thankfully, the API had all the
data I trained my model on. I had a drop-down to choose the airport, so I then converted the airport into location data for OpenWeather&amp;rsquo;s format
and then parsed the JSON response to get all the necessary values to pass into the correct model. Finally, I could hook all of this up to a
&lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt; front end. Gradio is made for easy front ends for AI applications, which I could easily publish on HuggingFace
so that anyone could interact with it as part of their &amp;ldquo;Spaces&amp;rdquo; feature. As all source code is available on HuggingFace spaces, I needed to add
an OpenWeather API input field, but by hosting it on HuggingFace anyone can still use the app on the web.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/flight-sure/flightsure_gradio.png&#34; alt=&#34;Airline On Time Statistics and Delay Causes, Jan 2021 to Dec 2023.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/flight-sure/flightsure_gradio.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;Airline On Time Statistics and Delay Causes, Jan 2021 to Dec 2023.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This was my project for my Data Mining class at San Jose State University. I made a slide show to show off the work to the class, which I attached below.
The main point of the class was finding the databases and cleaning the data to be used for modeling. I built the fun little Gradio app on top, but doing that is
a necessity when showing off a work like this.&lt;/p&gt;
&lt;p&gt;I don&amp;rsquo;t think the skills I learned in this project are that notable. Anyone in AI needs to dig through data and
Gradio is an industry standard for being an easy framework for non-UI developers. It was, though, a critical step in my education,
proving I know how to do these things. While I don&amp;rsquo;t expect this application to be used by anyone, it lays the groundwork of
my knowledge so that when I approach future machine learning problems that are novel and complex,
I have the work I did here in the back of my mind as reference for how to solve tomorrows problems.&lt;/p&gt;
&lt;iframe src=&#34;https://1drv.ms/p/c/78ac3e9c395f4290/IQRg-yVvnKWfQqwF6b_NDx-2AdW-ZZMH-EhxDafEj8CuoEQ?em=2&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;100%&#34; height=&#34;400px&#34; frameborder=&#34;0&#34;&gt;This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office&lt;/a&gt;.&lt;/iframe&gt;
&lt;div style=&#34;margin: 2rem 0;&#34;&gt;&lt;/div&gt;
&lt;p&gt;The app is available on HuggingFace at: &lt;a href=&#34;https://huggingface.co/spaces/Jaspann/FlightSure&#34;&gt;https://huggingface.co/spaces/Jaspann/FlightSure&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The dataset I used it on Kaggle at: &lt;a href=&#34;https://www.kaggle.com/datasets/williamparker20/flight-ontime-reporting-with-weather/data&#34;&gt;https://www.kaggle.com/datasets/williamparker20/flight-ontime-reporting-with-weather/data&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Mediapipe Pose Detection: Flutter Library</title>
      <link>http://localhost:1313/post/software-libraries/google-mediapipe-pose-detection/</link>
      <pubDate>Sun, 10 Mar 2024 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/software-libraries/google-mediapipe-pose-detection/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;A Flutter library for pose detection using Google&amp;rsquo;s Mediapipe.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Before 2023, Google&amp;rsquo;s solution for machine learning (ML) on mobile was
&lt;a href=&#34;https://developers.google.com/ml-kit/&#34;&gt;ML Kit&lt;/a&gt;. The library was for Android and iOS
and had a variety of small models to perform common tasks that developers could use for
their apps. This was before LLMs took over the public mind, so the platform heavily focused
on computer vision for tasks like facial detection and image labeling.
The library was well received by many developers and the developer community was even
interested enough in the library that it was ported to Flutter for
iOS and Android as &lt;a href=&#34;https://pub.dev/packages/google_ml_kit&#34;&gt;google_ml_kit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In 2023, Google re-introduced their on-device ML models under the upgraded
&lt;a href=&#34;https://developers.googleblog.com/en/introducing-mediapipe-solutions-for-on-device-machine-learning/&#34;&gt;Mediapipe Solutions&lt;/a&gt;
branding. They made this change as the library supported mobile as well as web and Python,
and they overhauled several models and took them out of beta. It appeared that Google
updated &lt;a href=&#34;https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker&#34;&gt;pose detection&lt;/a&gt;
as well, which was the model I used for FreeMoveVR. I tested out their
&lt;a href=&#34;https://github.com/google-ai-edge/mediapipe-samples/tree/main/examples/pose_landmarker/android&#34;&gt;Android app example&lt;/a&gt;,
and it felt much smoother than what I was able to achieve in my Flutter application.
The points looked less jittery and it was running at 30 fps, rather than my Flutter
app at 24 fps. I decided I wanted to re-implement google_ml_kit using Mediapipe
instead to try to improve my app.&lt;/p&gt;
&lt;h2 id=&#34;data-pipeline&#34;&gt;Data Pipeline&lt;/h2&gt;
&lt;p&gt;As Flutter is an abstracted layer on top of the native platform, we need a special way to
pass data between these, which is normally done through platform (aka method) channels.
Anytime a developer needs to interact with native APIs, it needs to take multiple steps: Flutter →
methodChannel → platform host → app delegate → Flutter controller → native API, and
back to return the data. The data is always passed asynchronously.&lt;/p&gt;
&lt;p&gt;Here is the image used to explain how this is done from
&lt;a href=&#34;https://docs.flutter.dev/platform-integration/platform-channels&#34;&gt;Flutter&amp;rsquo;s website&lt;/a&gt;:&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  class=&#34;bordered-image&#34;  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/software-libraries/google-mediapipe-pose-detection/PlatformChannels.png&#34; alt=&#34;Architectural overview: platform channels&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/software-libraries/google-mediapipe-pose-detection/PlatformChannels.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;Architectural overview: platform channels&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h3 id=&#34;data-throughput-in-m_ble_peripheral&#34;&gt;Data Throughput in m_ble_peripheral&lt;/h3&gt;
&lt;p&gt;I had previous experience with making a Flutter library in
&lt;a href=&#34;http://localhost:1313/post/software-libraries/m-ble-peripheral/&#34;&gt;m_ble_peripheral&lt;/a&gt;, but this library
was significantly different due to the quantity of data that needed to constantly
be transferred between the main application and the native code through the library.&lt;/p&gt;
&lt;p&gt;In Bluetooth Low Energy (BLE), the size of the data transferred are measured in bytes.
The messages sent to and from the app are only 20 bytes at max. The actual messages
are a bit more, but that is BLE headers handled by the operating system and not viewable
to the end application. There can be hundreds of messages sent a second, but that is
still almost nothing.&lt;/p&gt;
&lt;p&gt;The platform channels also have a limited number of supported data types. It&amp;rsquo;s mostly just
your standard data types in addition to &lt;code&gt;null&lt;/code&gt;, some &lt;code&gt;List&lt;/code&gt;s and &lt;code&gt;Map&lt;/code&gt;. This is really easy to
work with for m_ble_peripheral because one of the types is a &lt;code&gt;Uint8List&lt;/code&gt; that can easily
represent notification data and UUIDs can be &lt;code&gt;String&lt;/code&gt;s.&lt;/p&gt;
&lt;p&gt;Unlike m_ble_peripheral, this library would be pushing the limits on what Flutter
and Mediapipe could handle real time. This was because of two factors: multiple
platform jumps with complex data type augmentation and Mediapipe&amp;rsquo;s model execution time.&lt;/p&gt;
&lt;h3 id=&#34;data-throughput-for-mediapipe&#34;&gt;Data Throughput for Mediapipe&lt;/h3&gt;
&lt;p&gt;As we just discussed, there is a pipeline every time we need to transfer data,
and there is a limited number of data types we can pass through. This creates
a problem for video streams. Video streams contain lots of data, and need high
throughput. If we are operating at 30 fps, we have less than 0.033 seconds to
process data and get a result before the next frame is expected to come in. I
say less than because frames may not be captured at exactly the same rate,
especially if the phone is doing a lot of processing like it would be when
running the pose detection model.&lt;/p&gt;
&lt;h4 id=&#34;framerate-on-android&#34;&gt;Framerate on Android&lt;/h4&gt;
&lt;p&gt;While debugging, I found an even greater disconnect with this intuition.
While iOS can run 30 fps, I could not find any phone model on Android that
ran 30 fps. Back when I was starting the app in the fall of 2022, I took a
survey and ran FreeMoveVR on several of my friend&amp;rsquo;s phones:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Phone&lt;/th&gt;
          &lt;th&gt;Year&lt;/th&gt;
          &lt;th&gt;Avg. FPS&lt;/th&gt;
          &lt;th&gt;Version&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Moto g6&lt;/td&gt;
          &lt;td&gt;2018&lt;/td&gt;
          &lt;td&gt;9.0&lt;/td&gt;
          &lt;td&gt;9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Pixel XL&lt;/td&gt;
          &lt;td&gt;2018&lt;/td&gt;
          &lt;td&gt;7.4&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;OnePlus 7T Pro 5G&lt;/td&gt;
          &lt;td&gt;2019&lt;/td&gt;
          &lt;td&gt;17.0&lt;/td&gt;
          &lt;td&gt;11&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Samsung Gal S21 Ultra&lt;/td&gt;
          &lt;td&gt;2021&lt;/td&gt;
          &lt;td&gt;22.8&lt;/td&gt;
          &lt;td&gt;?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Samsung Gal 10&lt;/td&gt;
          &lt;td&gt;2019&lt;/td&gt;
          &lt;td&gt;16.4&lt;/td&gt;
          &lt;td&gt;?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Samsung Gal 10+&lt;/td&gt;
          &lt;td&gt;2019&lt;/td&gt;
          &lt;td&gt;15.3&lt;/td&gt;
          &lt;td&gt;?&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Samsung Gal S21&lt;/td&gt;
          &lt;td&gt;2021&lt;/td&gt;
          &lt;td&gt;23.9&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Google Pixel 4a 5G&lt;/td&gt;
          &lt;td&gt;2020&lt;/td&gt;
          &lt;td&gt;23.5&lt;/td&gt;
          &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you can see, no model was capable of running at over 24 frames per second.
While the average phone has increased in capabilities, any phone is able to
normally record a 30 fps video. This led me to the internal code of Flutter&amp;rsquo;s
&lt;a href=&#34;https://pub.dev/packages/camera&#34;&gt;camera plugin&lt;/a&gt;. This is the standard camera
plugin for Flutter, made by the Flutter development team. During my time
developing, the camera plugin in the backend used the outdated
&lt;a href=&#34;https://pub.dev/packages/camera_android&#34;&gt;Camera 2 library&lt;/a&gt; for Android.
With their solution, the frames that are meant to be viewed and the data as
it exists to be sent for further processing are not equivalent. While the
real framerate of the camera might be a full 30 fps, and the user sees 30 fps
when doing a visual test, some frames are dropped when sending to the callback
function I use to process the image. Because of this, I was never able to see
above 24 fps in testing on Android. While this made my goal of 30 fps in this
library impossible without significantly changing Flutter&amp;rsquo;s code, I decided to
continue to work on this library to use the improved model and framework.&lt;/p&gt;
&lt;h4 id=&#34;processing-images&#34;&gt;Processing Images&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s now return to thinking about platform channels. When my Flutter app needs
to process the camera image, it needs to take many steps before it is sent to
Mediapipe. The camera library runs Android&amp;rsquo;s native Camera library, Camera 2,
Camera 2 returns the camera as a viewable image and a version that is made for
processing, Both of these formats then need to be converted into a valid data
type to be transferred over the platform channel, the Flutter Camera library
then needs to re-create those images, One copy is displayed on the screen and
the other is sent to my callback. The callback now needs to perform that process
again in reverse to go from the Flutter side of my library to the respective
platform&amp;rsquo;s native side. Then, Mediapipe does not natively support the image type
that Android produces when streaming in preview mode, so we need to convert
the YUV image to Bitmap. It is actually really hard to go from YUV directly to
Bitmap, so actually we need to first convert from YUV to JPEG, then JPEG to
Bitmap. Once it is in the correct image format, then we can finally send the
image through Mediapipe to get our pose data.&lt;/p&gt;
&lt;p&gt;While this is a complex process, it is no ware near deal breaking for our
solution. From what I remember, once the image was captured by Camera 2,
all of this could be done in less then 2 milliseconds, or 0.002 seconds
at 240p. The main bottleneck was due to converting between image formats at the end.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Side note: 240p sounds low, but low qualities like this are actually normal input for
pose detection models as higher qualities take longer for the model to process.
All of Google&amp;rsquo;s pose detection models have 224 x 224-pixel input.
Additionally, for pose recognition, the person normally takes up a large
portion of the image and is in an unambitious position, so the model
would likely only improve slightly but take exponentially longer for larger
inputs.&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;running-the-mediapipe-model&#34;&gt;Running the Mediapipe Model&lt;/h2&gt;
&lt;p&gt;After all this research and work, interacting with Mediapipe was relatively easy.
I added a builder to set up the &lt;code&gt;PoseLandmarker&lt;/code&gt; object, which only needs a few
enum values. We can then pass through our image data, and the model runs
asynchronously. When the model is done, we can send the result to
&lt;code&gt;EventChannel.EventSink&lt;/code&gt;, where my library and app are listening for updates.
Mediapipe simply has a &lt;code&gt;setResultListener&lt;/code&gt;, and from there we format the
data in the same way as &lt;code&gt;google_ml_kit&lt;/code&gt; to maintain compatibility for users.&lt;/p&gt;
&lt;p&gt;This is the first time I had a memory leak in my software, so I needed to learn how
to debug that. Xcode has a tool to find memory leaks that I was able to pretty quickly
understand and use. I ended up finding the cause of it in my register function. I had it end
by kicking off a &lt;code&gt;DispatchQueue&lt;/code&gt; that would infinity run so it could detect the image as soon
as possible. While this does not look good, in my defense, I was still new to Swift so I
based the code on my Android implementation, where running the function in a separate
thread is a valid solution. Instead, I had the detect function run via &lt;code&gt;DispatchQueue&lt;/code&gt;
only after we had a result. From what I remember, I tried to mirror this on the Android
side but it would not work, and doing it as I described in a thread was the best solution.&lt;/p&gt;
&lt;p&gt;Now we could ask the question of which model should be used when for Mediapipe Pose
detection. Mediapipe has three models, Lite, Full, and Heavy. For my Flutter library,
all three are available of course, as it is as simple as downloading the model and
changing an enum in the builder. From my personal experience, the Lite
model is almost unusable. It is a surefire way to hit max framerate, but it was
struggling to accurately detect my legs and arms even in optimal conditions when
I was standing still. I&amp;rsquo;m honestly not sure what the use case even is with this quality.
Heavy is quite good, but the issue is that it would never come
close to a usable framerate for my application. I am sure it is very useful for images
and apps that do not require a real-time response, but I could use it for FreeMoveVR.
The Full model is perfect for any live feed application.
It balances quality and speed very well for modern phones, often getting near full
fps on 2020 and newer.&lt;/p&gt;
&lt;h2 id=&#34;retrospective&#34;&gt;Retrospective&lt;/h2&gt;
&lt;p&gt;I am pretty happy with how the library turned out. The only improvement I think I would
make looking back was setting up &lt;a href=&#34;https://github.com/dependabot/dependabot-core&#34;&gt;Dependabot&lt;/a&gt;
so my library could update automatically. Other then that, I happily use the library for
my own projects. I felt like this was a significancy more serious library for Flutter
then when I built &lt;code&gt;m_ble_peripheral&lt;/code&gt;. Even if it is not that complex, deigning and
implementing a need I had in this way significantly helped my understanding of Flutter
and library development as a whole.&lt;/p&gt;
&lt;p&gt;The code is available at: &lt;a href=&#34;https://github.com/FreeMove-VR/google_mediapipe_pose_detection/tree/main&#34;&gt;https://github.com/FreeMove-VR/google_mediapipe_pose_detection/tree/main&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Peripheral BLE Library for Flutter</title>
      <link>http://localhost:1313/post/software-libraries/m-ble-peripheral/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/software-libraries/m-ble-peripheral/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;Developing a Flutter library with full BLE peripheral capabilities for iOS and Android&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;bluetooth-for-freemovevr&#34;&gt;Bluetooth For FreeMoveVR&lt;/h2&gt;
&lt;p&gt;Early on in the creation of FreeMoveVR, I knew I wanted the primary communication
method between the smartphone and computer to be over Bluetooth. I believed that
Bluetooth was the perfect solution for our use case because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Everyone has Bluetooth on their phone, and it is a stable option for developers.&lt;/li&gt;
&lt;li&gt;FreeMoveVR needs to transfer a relatively small amount of data compared to something like streaming music to AirPods.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s wireless, so there is less hassle setting up the program for the end user.&lt;/li&gt;
&lt;li&gt;It does not rely on Wi-Fi at all, which is good in cases like colleges where the protocols are very complex and locked down for students.&lt;/li&gt;
&lt;li&gt;BLE stands for Bluetooth Low Energy, so the implication is that the phone can do the main task of pose detection and spend minimal resources on data transfer.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The most notable arguments against the other methods were that a wired connection would be hard to use given how the app works,
and that a Wi-Fi connection would be impossible to develop while on campus at SNHU.&lt;/p&gt;
&lt;p&gt;Working on FreeMoveVR was the first time I ever looked into Bluetooth as a developer.
When learning about the technology, there seemed to be a lot of missing information in discussions, so I will
briefly discuss some core background to understand the problem I was facing.&lt;/p&gt;
&lt;h2 id=&#34;bluetooth&#34;&gt;Bluetooth&lt;/h2&gt;
&lt;p&gt;When first learning about Bluetooth, there are two main types developers may be talking about:
&lt;a href=&#34;https://developer.android.com/develop/connectivity/bluetooth/ble/ble-overview&#34;&gt;Bluetooth Low Energy&lt;/a&gt;
(BLE) and Classic Bluetooth, often associated with Bluetooth versions like Bluetooth 5.
Classic Bluetooth is what something like AirPods or your car uses to transfer large amounts
of data and something that needs to be manually set up on your phone.
Advanced applications with heavy data transfer likely need Classic Bluetooth.
If your application has lower data streaming needs, you can likely use BLE.
BLE allows for easier and even automatic connections.
BLE facilitates easier and even automatic connections. An app with BLE functionality
can scan for known UUIDs, allowing it to connect to devices and read static or streaming data.
This is the basic concept behind Bluetooth mesh networks, which is sometimes brought up in smart homes or offices.
There are also many more libraries for BLE than Bluetooth 5. A very good library that I use for desktop applications is
&lt;a href=&#34;https://www.simpleble.org/&#34;&gt;SimpleBLE&lt;/a&gt;, and for Python &lt;a href=&#34;https://github.com/hbldh/bleak&#34;&gt;Bleak&lt;/a&gt; is very popular.
Overall, BLE is the correct choice for most small to medium projects for the ease of use for developers and users.&lt;/p&gt;
&lt;p&gt;BLE works on a central &amp;amp; peripheral system. This is loosely similar to the concept of a client and server.
The peripheral &amp;ldquo;advertises&amp;rdquo; the BLE connections that centrals can connect to. Each connection has two UUIDs
one for the service (normally one per application) and one for the characteristic, which an application
can have as many as needed. When a central sees UUIDs it recognizes, it can connect and start reading and
writing data from the peripheral.&lt;/p&gt;
&lt;p&gt;The main issue that I faced in development was that BLE libraries and developers almost always focus
on the central and rarely implement the peripheral side of the technology. &lt;a href=&#34;https://github.com/kevincar/bless&#34;&gt;Bless&lt;/a&gt;
is another Python library for BLE that implements the peripheral side of BLE,
but only has 127 stars on GitHub and is hardly maintained compared to Bleak
with 1.9k stars and a much more active commit history. SimpleBLE only supports central
and the peripheral side has been in development for &lt;a href=&#34;https://github.com/simpleble/simpleble/issues/99&#34;&gt;2.5 years&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;ble-and-flutter&#34;&gt;BLE and Flutter&lt;/h2&gt;
&lt;p&gt;Flutter has one of the best &lt;a href=&#34;https://pub.dev/&#34;&gt;communities for libraries&lt;/a&gt;.
It is much smaller than that of Python or NPM, but you can still find almost anything you need for mobile development.&lt;/p&gt;
&lt;p&gt;When I was initially building FreeMoveVR in 2022, I only focused on building for Android,
and I would retrofit iOS compatibility later. I was not too worried about trying to make
sure what I did would instantly work on iOS, but I chose Flutter for the platform and
tried to make decisions to make iOS compatibility easier later on.&lt;/p&gt;
&lt;p&gt;When going over the requirements at the time I had found something interesting.
Multiple libraries existed for the central side of BLE, and some even existed for
implementing the peripheral side on
&lt;a href=&#34;https://github.com/keysking/k_ble_peripheral&#34;&gt;Android devices&lt;/a&gt;, but none existed for iOS.
It was even more interesting, as the libraries at the time said that iOS was not planned or
possible, even though it was clearly possible on
&lt;a href=&#34;https://developer.apple.com/documentation/corebluetooth/cbperipheral&#34;&gt;Apple&amp;rsquo;s documentation&lt;/a&gt;
There were a lot of different parts of FreeMoveVR I needed to work on and I did not have any
Apple hardware available to me at the time, so I used an Android-exclusive peripheral library
and even &lt;a href=&#34;https://github.com/keysking/k_ble_peripheral/pull/5&#34;&gt;fixed a small&lt;/a&gt; issue I had
noticed when using it.&lt;/p&gt;
&lt;p&gt;By September 2023, it had been a year since I was aware of the issue in my app, and I
was not seeing any progress in the Flutter space for enabling BLE support.
I decided it was time that I started working on modifying the library I was using to add iOS support.
As the owner seemed to have abandoned the project, I forked it for my use case.&lt;/p&gt;
&lt;p&gt;I was not aware, but at the same time, a package simply called
&lt;a href=&#34;https://pub.dev/packages/bluetooth_low_energy&#34;&gt;bluetooth_low_energy&lt;/a&gt; was about to be published to
pub.dev which today does exactly what I needed at the time.&lt;/p&gt;
&lt;h2 id=&#34;m_ble_peripheral&#34;&gt;m_ble_peripheral&lt;/h2&gt;
&lt;p&gt;The project I forked was called &lt;a href=&#34;https://github.com/keysking/k_ble_peripheral&#34;&gt;k_ble_peripheral&lt;/a&gt;.
I&amp;rsquo;m not sure why &amp;lsquo;k&amp;rsquo; was in the name, but I decided my fork would replace the &amp;lsquo;k&amp;rsquo; with a &amp;rsquo;m&amp;rsquo; to
signify I modified it, making it &lt;a href=&#34;https://github.com/Jaspann/m_ble_peripheral&#34;&gt;m_ble_peripheral&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This was my first time working on a Flutter library for iOS, or really at all.
I was only making the project for FreeMoveVR, so I had a very clear idea of the
features I needed to add and what I didn&amp;rsquo;t need to think about.
This was also the main reason I didn&amp;rsquo;t publish the library on pub.dev,
as I didn&amp;rsquo;t really mind if the library did not implement all the features available to
Apple phones or meet feature parity with the Android side.
I only needed enough iOS code for FreeMoveVR to communicate with the desktop, and then I
could consider this complete.&lt;/p&gt;
&lt;p&gt;As someone that never used Apple products before developing for FreeMoveVR, I was surprised
by the limitations they impose on developers and superusers. I was trying to download
Flutter library example apps to understand the process, and quickly hit the three-app limit
with code I was actively using to build the library for the platform. The way that Flutter needs
to handle itself inside Xcode is also unorthodox, according to the
&lt;a href=&#34;https://docs.flutter.dev/packages-and-plugins/developing-packages#step-2c-add-ios-platform-code-swift-hplus-m&#34;&gt;Flutter doc&lt;/a&gt;
to start development on a package, you need to locate your files Xcode at:
&lt;code&gt;Pods/Development Pods/examplePlugin/../../example/ios/.symlinks/plugins/examplePlugin/ios/Classes&lt;/code&gt;
This is beyond strange to have required &lt;code&gt;..&lt;/code&gt; in the file path, as that means that you would land
yourself back at &lt;code&gt;Pods&lt;/code&gt;, but this whole path is required to get to &lt;code&gt;example&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As someone who learned software development primarily through the Android ecosystem, it was a
breath of fresh air looking at Apple&amp;rsquo;s documentation for BLE. I sometimes think that the documentation
in Android feels so ambiguous that it was automatically generated using only the name of the function as
a reference. Apple, on the other hand, not only has descriptions for classes and details about functions
that I found useful, but they even had an example project for Macs! As someone who never had written in Swift
before this, I felt like I was able to perfectly understand the library and how I needed to implement it.&lt;/p&gt;
&lt;p&gt;The main unique thing I noticed when coding in Swift is its extra keyword called &lt;code&gt;guard&lt;/code&gt;. This is used for
negative conditions, where we need to check if the values are valid before we run a method. This is great, as
it allows you to check your inputs like you would in an &lt;code&gt;if&lt;/code&gt; statement but in a more streamlined way.&lt;/p&gt;
&lt;h2 id=&#34;development&#34;&gt;Development&lt;/h2&gt;
&lt;p&gt;I knew that the structure for k_ble on Android worked and saw many similarities between the two platforms
BLE libraries, so I thought it would be best to go in with a general structure mirroring the Android side in
addition to the example provided by Apple. I needed the Flutter code to be as platform-agnostic as possible,
so taking hints from the Android side would help with that goal.&lt;/p&gt;
&lt;p&gt;Almost all my code ended up in &lt;code&gt;CharacteristicDelegate.swift&lt;/code&gt; and &lt;code&gt;PeripheralManagerHandler.swift&lt;/code&gt;. The other files,
like in Android, were for class structures and not much else. In the library, the main thing I needed to be aware of
was managing the active connections, as we abstracted that away in the Flutter layer,
but it is handled by the program at least on the Apple side. That meant managing objects that the app found and
connected to in a way that would be standard for libraries.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This was a small project I did 2 years ago, and I didn&amp;rsquo;t end up leaving comments.
So, like most of these posts, I am piecing back together what happened as I write the post.
While I am happy I made the project, I genuinely don&amp;rsquo;t remember much about the problems I encountered,
and I don&amp;rsquo;t think it is a good use of time to study my code to figure out what I thought I did back then.
I would have liked to make the development section much larger, and I may in the future, but understanding
at a macro level is becoming more important as time progresses, so I am fine leaving the post as is for now.
If you want to look at the code though, it is available at:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Jaspann/m_ble_peripheral&#34;&gt;https://github.com/Jaspann/m_ble_peripheral&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Balancing the backward compatibility with k_ble and the differences between iOS and Android&amp;rsquo;s implementation on the
BLE peripheral specification was an interesting and fun challenge. From this, I learned a lot about how Flutter&amp;rsquo;s libraries
work, Bluetooth, and backward compatibility with existing solutions. This immediately preceded my work on my next Flutter
library, google_mediapipe_pose_detection, where I further expanded my abilities in this area.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Eventified</title>
      <link>http://localhost:1313/post/mobile-projects/eventified/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/mobile-projects/eventified/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;An Android calendar app for university clubs.&lt;/em&gt;&lt;/p&gt;



&lt;div class=&#34;gallery caption-position-bottom caption-effect-slide hover-effect-zoom hover-transition&#34; itemscope itemtype=&#34;http://schema.org/ImageGallery&#34;&gt;
    
  
  &lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/css/hugo-easy-gallery.css&#34; /&gt;
  &lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34;
    itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
      &lt;div class=&#34;img&#34; style=&#34;background-image: url(&#39;/mobile-projects/eventified/androidStack.jpg&#39;);&#34;&gt;
        &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/mobile-projects/eventified/androidStack.jpg&#34; /&gt;
      &lt;/div&gt;
      &lt;a href=&#34;http://localhost:1313/mobile-projects/eventified/androidStack.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
    &lt;/figure&gt;
  &lt;/div&gt;

  
  
  &lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34;
    itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
      &lt;div class=&#34;img&#34; style=&#34;background-image: url(&#39;/mobile-projects/eventified/homescreen.png&#39;);&#34;&gt;
        &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/mobile-projects/eventified/homescreen.png&#34; /&gt;
      &lt;/div&gt;
      &lt;a href=&#34;http://localhost:1313/mobile-projects/eventified/homescreen.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
    &lt;/figure&gt;
  &lt;/div&gt;


&lt;/div&gt;

&lt;h2 id=&#34;arriving-at-snhu&#34;&gt;Arriving at SNHU&lt;/h2&gt;
&lt;p&gt;It was my first semester on campus at Southern New Hampshire University (SNHU).
Covid was the year prior, so my class along with the cohort below me was new to the campus.
I had experience with Java in my high school and even &lt;a href=&#34;http://localhost:1313/post/community-work/ap-comp-sci-a-ta/&#34;&gt;taught the AP course on it&lt;/a&gt;.
They also taught Java in the first coding class of SNHU,
so going into my second year I figured I would want to use it in team-based projects.&lt;/p&gt;
&lt;p&gt;SNHU is unique in that, even though the degree says I have a computer science degree,
it is taught much more like how I would imagine a software engineering degree to be taught.
Each semester (or year) you are put with a group to build a single project. It is a great exercise in
working as a software development team, developing using agile, using issue tracking boards,
and the teacher acts similarly to a project manager.&lt;/p&gt;
&lt;p&gt;I quickly realized, that, because of my extra AP credits moving me ahead in the graduation schedule,
I was placed in an extremely small class with all the students that either were like me and got
ahead somehow or fell behind over the pandemic.&lt;/p&gt;
&lt;h2 id=&#34;creating-the-idea&#34;&gt;Creating the Idea&lt;/h2&gt;
&lt;p&gt;My favorite part of the software engineering series of classes had to be the
start of the semester when we pitched our ideas. Students can choose almost anything they want
as long as it is with the teacher&amp;rsquo;s permission. It can be any problem, using any language or
framework, and as long as you pitch well and can prove that the goal is achievable within the year,
it is likely that you will get a team to work on the project with you.&lt;/p&gt;
&lt;p&gt;As a new student on campus, I had been extremely confused about how to engage in the community.
Club events often felt like they were not communicated well, and I felt like I missed out on a lot
after I did not get all the contacts I wanted after the school club fair. I wanted this to be my
problem statement for the class. I knew the class knew Java and Android apps are written in Java, so
it would be inclusive to the other members, and it was relatively simple in theory. We would need to have
some external server to sync all the data, but there are upperclassmen who offered help in that area
and we were learning about databases in another class.&lt;/p&gt;
&lt;p&gt;The teacher decided that partners were decided via how we chose to sit ourselves on the first day, and my
partner did not have any ideas from what I remember. The teacher begrudgingly accepted my idea. I found out
later many students made calender apps to the point he was going to ban the idea after my semester. The other
team decided to take the teacher&amp;rsquo;s pet idea, which created a clear favorite from the teacher.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;This was probably the semester I worked the hardest in school because of this class. Even knowing the language,
I found that knowing the framework was often the larger hurdle. My only experience was with the raw languages and
Unity, so working within the Android ecosystem was a lot all at once. I needed to make sure the work I was doing
matched the teacher&amp;rsquo;s expectations: my partner found the topic extremely difficult, the teacher was constantly
unimpressed with the project, and I needed to not only learn Android but also get it working with AWS.&lt;/p&gt;
&lt;h3 id=&#34;frontend&#34;&gt;Frontend&lt;/h3&gt;
&lt;p&gt;Any type of UI solution that Android offered was going to be new to me. For &lt;a href=&#34;http://localhost:1313/post/mobile-projects/pond-jumper/&#34;&gt;Pond Jumper&lt;/a&gt;
I only needed a few buttons for a game while Eventified a end user application that had much more intricate design over several
pages that needed to get content from an external source. In 2021, &lt;a href=&#34;https://developer.android.com/compose&#34;&gt;Android Jetpack Compose&lt;/a&gt;
was very recently released, and not many tutorials explained it well. Instead, we used the more stable XML UI solution.&lt;/p&gt;
&lt;p&gt;Android uses custom XML elements to define each of its components that are in the framework by default in addition to your own custom
components. All the attributes are defined by XML, then if you need to override them you do so in the code, like grabbing a logo for
our club information. This is very clear to understand in my opinion, but it is easy to see why Google moved away from this solution as
XML limits the options and splits the development into two languages, whereas Jetpack Compose allows developers to write everything in Kotlin.&lt;/p&gt;
&lt;p&gt;One of the largest issues I remember facing in the front end was that RecyclerView, responsible for scrolling items past the screen, was not
working for my requirements. If I recall correctly, this was because RecyclerView only handles one class type, while I needed it to handle
two for both the student&amp;rsquo;s personal events as well as the club events. Handling this is surprisingly challenging. I needed to extend RecyclerView.Adapter
&lt;a href=&#34;https://github1s.com/Jaspann/Eventified/blob/HEAD/app/src/main/java/com/example/eventified/HomeAdapter.java&#34;&gt;creating a new class&lt;/a&gt;
that overrode &lt;code&gt;onBindViewHolder&lt;/code&gt;, forcing it to figure out which type of event it was and correctly applied to the data. I was able to get away with
two classes that &lt;code&gt;extends RecyclerView.ViewHolder&lt;/code&gt;, and did not need to do anything to PersonalEvent and ClubEvent, which in retrospect may
have been the cleaner solution.&lt;/p&gt;
&lt;p&gt;While that was the most notable issue I had, as I think it was the last roadblock we had in the project, I am surprised about how much I was able to do.
I very much agree with anyone saying that the UI does not look great. And it didn&amp;rsquo;t need to be for a group of two computer science students. But the code
is actually much more than I would have expected of myself in 2021. My commits back then were even larger than they are today, which I guess is some progress,
but I impressed myself with how much code there is for the semester.&lt;/p&gt;
&lt;p&gt;Reading through the code, there are also some interesting choices I made back then. I think most interestingly, I remember I really didn&amp;rsquo;t want the long URLs to
be in the middle of code blocks. This has some merit, as long URLs take up screen space making the code less readable. My solution was to
&lt;a href=&#34;https://github.com/Jaspann/Eventified/blob/master/app/src/main/java/com/example/eventified/MainActivity.java#L101&#34;&gt;add all of the URLs into the save data for the user on startup&lt;/a&gt;.
The idea of what to do is there, but there almost certainly was a better way to manage this.
Another thing was that, for a small amount of time, I thought that
AWS Amplify was the correct direction to go in instead. I only thought about this for a few days,
but at the end of the project, I still had references to Amplify in the code base.
Some level of cleaning up the code and making sure that everything looks nice goes a long way.
Another thing that goes a long way is comments. I had virtually no comments,
which was a serious problem of mine back then. Today I at least try to add doc
comments to the code I edited at the end of a commit or branch, often requiring it in the linter,
and comment whenever standard convention is broken.&lt;/p&gt;
&lt;h3 id=&#34;backend&#34;&gt;Backend&lt;/h3&gt;
&lt;p&gt;The only real requirement that the teacher imposed on our projects were that we needed to connect to an external SQL database to store data.
This was fine for our project, and it was there so that we couldn&amp;rsquo;t cheat him and say that we are storing all the data locally.&lt;/p&gt;
&lt;p&gt;While the other group went to an upperclassman for help with the server, I was of the opinion that their solution was too much,
work for the small amount of processing that needed to be done for the problem we were tackling,
and looked into a serverless solution instead. I didn&amp;rsquo;t know much at the time, but all I knew I needed was a few functions
that needed to be activated on the backend which looked exactly like what &lt;a href=&#34;https://aws.amazon.com/lambda/&#34;&gt;AWS Lambda&lt;/a&gt; was offering.
It required Python, but it allowed me to access the backend database from my app which was enough for the project.&lt;/p&gt;
&lt;p&gt;I set up endpoints via &lt;a href=&#34;https://aws.amazon.com/api-gateway/&#34;&gt;Amazon API Gateway&lt;/a&gt; for the app to activate the lambda functions.
API Gateway was just a simple UI for creating random URLs for the project and hooking them up to your Lambda functions easily.
The API Gateway would pass the JSON payload to the AWS lambda function, which did some simple checking that the account doing the action is valid, and
then doing simple CRUD operations on the SQL database before sending the data back. As everything was very simple,
I was able to get away with using Python without any libraries, which was a decent introduction to Python for me.&lt;/p&gt;
&lt;p&gt;Frontend then needed to pass the data to the backend using a library called &lt;a href=&#34;https://google.github.io/volley/&#34;&gt;Android Volley&lt;/a&gt;.
Volley is a simple HTTP library, which was all we needed for this application. Volley handles requests in kind of a unique way, where
instead of &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt;, you create a request queue that is processed through. In practice this means we never &lt;code&gt;await&lt;/code&gt;,
so we need to heavily use live updating instead, which was fine for our use case.&lt;/p&gt;
&lt;h2 id=&#34;final-product&#34;&gt;Final Product&lt;/h2&gt;
&lt;p&gt;Here is the final visualization we made for the class to explain the tech stack.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/mobile-projects/eventified/androidStack.jpg&#34; alt=&#34;A visualization of the stack used. On Front end, we made the app for Android using Android Studio and Java, passed through API calls via the Android Volley library which went to AWS&amp;#39;s API Gateway, which activated a Lambda function written in Python to finally preform an operation on the SQL database, and send the information back up the chain.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/mobile-projects/eventified/androidStack.jpg&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;A visualization of the stack used. On Front end, we made the app for Android using Android Studio and Java, passed through API calls via the Android Volley library which went to AWS&amp;#39;s API Gateway, which activated a Lambda function written in Python to finally preform an operation on the SQL database, and send the information back up the chain.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;In addition, we needed to present our work in front of the CS department.
For this, we made a small YouTube video to see the flow of the app and
a slideshow showing what we did for the class.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/53W1g1DNmHY?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;div style=&#34;margin: 2rem 0;&#34;&gt;&lt;/div&gt;
&lt;iframe src=&#34;https://1drv.ms/p/c/78ac3e9c395f4290/IQT4glrVFTcxR6_8_9ZO6__jAQPDLy0OEk9MHgNe8nHEhRI?em=2&amp;amp;wdAr=1.7777777777777777&#34; width=&#34;100%&#34; height=&#34;400px&#34; frameborder=&#34;0&#34;&gt;This is an embedded &lt;a target=&#34;_blank&#34; href=&#34;https://office.com&#34;&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=&#34;_blank&#34; href=&#34;https://office.com/webapps&#34;&gt;Office&lt;/a&gt;.&lt;/iframe&gt;
&lt;div style=&#34;margin: 2rem 0;&#34;&gt;&lt;/div&gt;
&lt;p&gt;At the end of the semester, we were happy with where the app was. We understood how to develop
a consumer-facing social application in Android, and wanted to move on to something else. The project
is still available on GitHub for anyone interested:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Jaspann/Eventified&#34;&gt;https://github.com/Jaspann/Eventified&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>3tene AutoHotKey</title>
      <link>http://localhost:1313/post/ai-projects/3tene-autohotkey/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/ai-projects/3tene-autohotkey/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;Allowing VTubers to express themselves through global hotkeys.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;finding-vtubing-and-3tene&#34;&gt;Finding VTubing and 3tene&lt;/h2&gt;
&lt;p&gt;Once the coronavirus pandemic hit and society was in lockdown, I started watching more Twitch.
Around this time, VTubing was taking off, as people could remain semi-anonymous online while
attempting to grow a following on video-based platforms. The concept was in good fun and not too serious
as you would expect from a mildly popular casual entertainment genre. This was my first look at human
positional tracking systems, and I was impressed with the underlying technology. The problem often was in
the feature set of the applications used for this growing genre of entertainment, where key features I
would expect to find in popularly recommended apps seemed to be missing for the technology to
be used more seriously.&lt;/p&gt;
&lt;p&gt;The app I grew affectionate towards was &lt;a href=&#34;https://3tene.com/&#34;&gt;3tene&lt;/a&gt;. 3tene was primarily meant for facial
tracking at the time and seemed to have a mix of customizability and community support. When I watched VTubers,
I noticed that the facial expressions sometimes changed with what they were saying, and I wanted to do
something like that in 3tene. While the feature existed in the app, I was disappointed that as soon as I
clicked on another window I could no longer use the shortcut to change my emotion. As a relatively new programmer,
I wanted to see if I could solve this problem.&lt;/p&gt;
&lt;p&gt;Side note: this is not really and AI project. With my post grouping structure,
this is the only post that did not fall cleanly into a category, so I put it under AI
as the script extends the abilities of an AI facial tracking app.&lt;/p&gt;
&lt;h2 id=&#34;solving-with-autohotkey&#34;&gt;Solving With AutoHotKey&lt;/h2&gt;
&lt;p&gt;The first thought I would expect most people to have comes down to: &amp;ldquo;Windows says &amp;lsquo;No&amp;rsquo; and 3tene says &amp;lsquo;No&amp;rsquo; so you
can&amp;rsquo;t program your way out of it.&amp;rdquo; That is a pretty fair response, as I can&amp;rsquo;t edit either of these pieces of software
myself, and creating a stand-alone app, which is all I knew how to do, does nothing in this problem. I only knew Java
and Unity, and nothing that I had learned in school felt like it could help me in any way. Thinking about
how to solve the problem, an old Tom Scott video came into mind for how to create duct-tape solutions in Windows.
&lt;a href=&#34;https://youtu.be/lIFE7h3m40U&#34;&gt;The Art of the Bodge: How I Made The Emoji Keyboard&lt;/a&gt; seemed to use
&lt;a href=&#34;https://www.autohotkey.com/&#34;&gt;AutoHotKey&lt;/a&gt; (AHK) for a similar type of otherwise impossible solution I was looking for.
There was good documentation for everything I could need and there was enough of a user base to find all of the questions I had online.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;As a novice programmer, I searched through Google to find something in AHK that could send keystrokes from anywhere on the
desktop into an application. The solutions online often did not work, as &lt;code&gt;ControlSend&lt;/code&gt; did not send the update to 3tene
unless the window was first focused. 3tene didn&amp;rsquo;t like the num pad much, which was annoying because that was how I assumed most
users would control their avatar. I found a working solution though over the course of a day or two and simply copied and pasted the
five lines a dozen times over to create a key bind for each number on the num pad. The solution worked, and that&amp;rsquo;s all
that needed to matter as a new developer.&lt;/p&gt;
&lt;h2 id=&#34;publishing&#34;&gt;Publishing&lt;/h2&gt;
&lt;p&gt;Once I wrote the script and verified that everything worked, I posted about it on&lt;br&gt;
&lt;a href=&#34;https://www.reddit.com/r/VirtualYoutubers/comments/ikxoq1/solution_to_using_hotkeys_without_focus_on_3tene/&#34;&gt;r/VirtualYoutubers&lt;/a&gt;
and 3tene&amp;rsquo;s &lt;a href=&#34;https://steamcommunity.com/app/871170/discussions/0/2945872608881921166/&#34;&gt;Steam Page&lt;/a&gt;. I got a few upvotes and
comments saying thanks, and I was happy that someone else was benefiting from what I made.&lt;/p&gt;
&lt;p&gt;A few months later, I found out that a YouTube video by Blossom Baphomet had made a tutorial on my script.
To my surprise, the video has garnered over 1.7 thousand views since its release,
exceeding all of my expectations for its reach when I first created it.&lt;/p&gt;
&lt;p&gt;Blossom Baphomet delisted her videos on her channel,
but I was granted permission to link the video for this post.&lt;/p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/OO6NMm2qoB8?autoplay=0&amp;amp;controls=1&amp;amp;end=0&amp;amp;loop=0&amp;amp;mute=0&amp;amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;&gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;h2 id=&#34;retrospective&#34;&gt;Retrospective&lt;/h2&gt;
&lt;p&gt;As this was the first software project I published on GitHub, I had some difficulties. The biggest issue I see looking
back on the project was the README file, as I treated it more like a dev log + instructional essay rather than a clear
and concise document for readers. I also should have tried harder for the script to be more user-friendly.
While the code is not long, users needed to input the shortcuts they want directly into the script,
and the code block needed to be copied each time the user wanted to add a new emotion to their character.
I should have made the code block into a function, and making the variables the user inputs into
a single data structure to manipulate.&lt;/p&gt;
&lt;p&gt;As I had effectively been self-taught via online classes and was completely unaware of coding outside of the
Eclipse Editor and the Unity engine, this project greatly expanded what I saw to be possible with programming.
Looking back, it was likely very beneficial to my outlook on software development that such a unique
problem and solution was the my first published project. It allowed me to be much more innovative when
creating project ideas, which in my opinion shows in the novelties of RoundHouse, Unified Live Chat, and FreeMoveVR.&lt;/p&gt;
&lt;h2 id=&#34;rewrite&#34;&gt;Rewrite&lt;/h2&gt;
&lt;p&gt;As part of this post, I decided I wanted to re-write the script with these edits in mind and see if my
original code even worked on modern software. Since its release, Windows 11 came out, AHK v2.0 was released,
and 3tene had many small updates over the years. When I tested it on AHK v1.1, to my surprise it actually
worked completely! AHK and Windows both pride themselves on backward completely, and there were never any
big overhauls in 3tene. I still wanted to make my changes so I started working on a AHK v2.0 version of the script.&lt;/p&gt;
&lt;p&gt;To start the re-write, the list of shortcuts was turned into a map, where users only needed to change one line to add their
key binds rather than copy and paste five lines and edit small components inside the code block. I added
some additional examples in the comments after the map for non-technical users in case they wanted to use the control, alt,
or shift in their key binds. Then, it was as simple as creating a function that directs input into the
window and sends the appropriate button, and then creating a for loop to apply the function to each hotkey in the map.&lt;/p&gt;
&lt;p&gt;I wanted to make sure that my README instructions also improved, so after re-writing, I had a non-technical friend
read over it and try to follow it. My friend may have been a bit &lt;em&gt;too&lt;/em&gt; non-technical, but from the exercise,
we cleaned up some ambiguities in the instructions. Additionally, a popup was added when the user enabled or disabled
the script for visual feedback that it was working.&lt;/p&gt;
&lt;p&gt;This is the first real project I made intended for others to use, and despite its size,
it holds a special place in my heart and defines how I approach software development to this day.&lt;/p&gt;
&lt;p&gt;The repo is available at &lt;a href=&#34;https://github.com/Jaspann/3tene-AutoHotKey&#34;&gt;https://github.com/Jaspann/3tene-AutoHotKey&lt;/a&gt;.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>AP Computer Science A Teacher&#39;s Assistant</title>
      <link>http://localhost:1313/post/community-work/ap-comp-sci-a-ta/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/community-work/ap-comp-sci-a-ta/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;By my senior year in high school, I had nearly complete freedom to do what I wanted,&lt;/em&gt;
&lt;em&gt;so I chose to help teach programming to my friends.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Most people can probably agree that high school can be as easy as you want it to be.
In my school, I needed a total of 24 credits. There were eight class slots per year.
One of those slots was often taken up by a free block, so the average student often
would gain seven a semester. Even if you complexly fail a class every semester,
you would still have six credits a year, multiplied by four years will give you the
24 credits needed to graduate. If you do well in school, you could easily skip the
free block and have eight credits a year, which means you would be theoretically able
to graduate a whole year early. Anyone that was that smart though would spend time
working on AP classes to get a head start in college.&lt;/p&gt;
&lt;p&gt;I had taken AP Computer Science A (APCSA) my junior year of high school over VLACS, as the class
was effectively self taught due to lack of teachers and limited demand for the class.
VLACS was an online curriculum which can substitute classes in your high school, and
unbeknownst to me it would become my favorite class would take in high school.&lt;/p&gt;
&lt;p&gt;My class felt heavily more academic than most years students, and that could be seen&lt;br&gt;
in the growth of students applying for APCSA my senior year. In my junior year
the teacher had &amp;ldquo;taught&amp;rdquo; 2 students, this time there were 15, all of which I would
consider my friends as we had a relatively small school. I decided to talk to the
computer science teacher at the school to judge if I should be a teacher&amp;rsquo;s assistant (TA),
and from what I remember he pretty much admitted that he did not have the ability to teach
the course as his job at the school consisted of teaching Scratch projects and extremely
basic Python for many years, and he was not aware of how the College Board&amp;rsquo;s AP tests worked.&lt;/p&gt;
&lt;p&gt;I was a good student. Not the best, but was still proud of my accomplishments because as I
stated my class felt extremely academic compared to average. None of the classes I wanted to
take interfered with the APCSA time slot, and I learned that both of the other students that took the class
with the teacher last year failed. Due to all this, I felt obligated to my friends to help teach
the course as a TA with this teacher.&lt;/p&gt;
&lt;h2 id=&#34;starting-teaching&#34;&gt;Starting Teaching&lt;/h2&gt;
&lt;p&gt;I knew I had to maintain an interesting balance of not overshadowing the teacher, teaching the students
what was needed for the test, and not turning my friendly relationships into ones of teacher and student.&lt;/p&gt;
&lt;p&gt;This was the first time I had taught others that actually depended on the accuracy of what I said.
Sure there were times in Boys Scouts or talking through what to do in a game, but nothing at this level.
This combination of figuring out how I wanted to teach for the first time and the interesting set of
social circumstances I was in allowed me to not only create a
unique teaching style that worked for me and my peers but also a love for teaching within myself.
I was taking a few other AP classes as well and found the lectures of those classes extremely boring with
little interaction and way too much memorization. I felt like I could have the students remember much more
if I taught in a more interactive way without trimming down on content. A metric I used was that I felt like I was doing
a good job if everyone agreed we were keeping on pace for the test while the class felt like a social break where we discussed
interesting topics and how to solve new problems that I could get the class invested in.&lt;/p&gt;
&lt;h2 id=&#34;just-a-chill-guy&#34;&gt;Just a Chill Guy&lt;/h2&gt;
&lt;p&gt;I would often discuss with the teacher what topics we should cover, and ask to present on them in class.
When I presented, instead of lecturing, I led a conversation with my students like an interactive live show,
where I would make sure my entire thought process of a problem was explainable in plain English, and talk through
solutions that everyone could get behind in theory
as we wrote them out and tested. Students could interject almost whenever they wanted, and I often asked for random
inputs from the students for critical components, which often ended up entertaining everyone including myself while
staying on track and learning about what we needed. I taught much of the course as thinking of physical objects that could
be manipulated though what we write. One of my favorite exercises was trying to explain sorting algorithms by
moving around playing cards, defining all of the possible moves in terms of what we learned so far in class, and letting the
students try to figure out the sorting algorithms themselves in a gamified way.&lt;/p&gt;
&lt;p&gt;Having in-class programming also was extremely fun for me and the students. We always did some sort of pair programming,
as it is much more fun talking and bouncing ideas off each other and it is good practice in the industry.
Additionally, I was able to see the wide variety of solutions that everyone was able to come up with. While the students
didn&amp;rsquo;t understand why I was taking so long at first, almost every pair had a different approach to the problem.
This showed me that programming is inherently creative. Technically, there is probably an optimal solution, but
like deciding on a JavaScript framework, where that type of discussion quickly becomes irrelevant most of the time.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;The entire year I had fun teaching the class and I felt like the students enjoyed coming every day.
The teacher loved all of the work and dedication I put in over the school year. While we did butt heads at times,
we were great teammates. My peers in my class also could sense my dedication and commitment to keep the class
light-hearted and bragged about being in my class or talked about how well I could explain concepts even when we
were with other students outside the class.&lt;/p&gt;
&lt;p&gt;Teacher&amp;rsquo;s assistants are discouraged from knowing the grades of the students they teach. Even so, I know that one
of my best friends from the class passed, and that he said the whole class had done reasonably well. The coronavirus
pandemic hit at the end of the year, so not much happened after the test, but at graduation I was awarded the top
computer science student of the year. An honor I was proud of receiving as a student that never took a programming
class at the school.&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Pond Jumper</title>
      <link>http://localhost:1313/post/mobile-projects/pond-jumper/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/mobile-projects/pond-jumper/</guid>
      
        <description>&lt;p&gt;&lt;em&gt;My First Large Project: an Arcade Game for Smart Phones.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Looking back at how I first learned software development, I feel very fortunate to have taken the path I did.
In my junior year of high school I took &lt;a href=&#34;https://apstudents.collegeboard.org/courses/ap-computer-science-a&#34;&gt;AP Computer Science A&lt;/a&gt;,
which was taught extremely thoroughly on &lt;a href=&#34;https://vlacs.org/&#34;&gt;VLACS&lt;/a&gt; and
is probably the best course I took in high school despite it being online. My senior year I needed to do a senior
project, and for this, I decided to re-implement an idea I had made previously, reimagining it using &lt;a href=&#34;https://unity.com/&#34;&gt;Unity&lt;/a&gt;.&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;http://localhost:1313/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/pond_jumper/pond_jumper_home.png&#34; alt=&#34;The Pond Jumper Title Screen.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/pond_jumper/pond_jumper_home.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;The Pond Jumper Title Screen.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&#34;game-inspiration&#34;&gt;Game Inspiration&lt;/h2&gt;
&lt;p&gt;At that time I had recently played &lt;a href=&#34;https://undertale.com/&#34;&gt;Undertale&lt;/a&gt;. The main action of the game is a bullet-hell system where you control
a small character that needs to move and avoid obstacles. While many aspects of Undertale are unique, a core part of
the game focuses on special mechanics in boss battles. During these scenes, the normal rules of the fights change and the
novelty of additional rules applied to the core bullet-hell mechanic are explored and become more complex throughout the battle.&lt;/p&gt;
&lt;p&gt;Two of the concepts I had wished there were more of were in the middle of the game, in the Undyne and Muffet battles. The
Undyne battle complexly changed the idea of a bullet-hell, where instead of avoiding the bullets, you had a shield and needed
to block them. The Muffet battle had the player jumping between pre-defined tracks to avoid the bullets which differed from the normal
game where you have complete control over the movement of your character to any point within the arena.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/pond_jumper/undyne_fight.png&#34; alt=&#34;The Undyne Fight in Undertale. Image source: DualShockers&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/pond_jumper/undyne_fight.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;The Undyne Fight in Undertale. Image source: DualShockers&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;div style=&#34;margin: 2rem 0;&#34;&gt;&lt;/div&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/pond_jumper/muffet_fight.png&#34; alt=&#34;The Muffet Fight in Undertale. Image source: Very Vennie&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/pond_jumper/muffet_fight.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;The Muffet Fight in Undertale. Image source: Very Vennie&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;While many bullet-hell styled games did exist, these takes on the genre rarely seemed to have been expanded upon.&lt;/p&gt;
&lt;h2 id=&#34;early-attempt&#34;&gt;Early Attempt&lt;/h2&gt;
&lt;p&gt;For a introductory computer class I took in high school my sophomore year, we were required to create a game in
&lt;a href=&#34;https://scratch.mit.edu/&#34;&gt;Scratch&lt;/a&gt;. At that time, I had wanted to learn to code, but genuinely had no clue about
how to get started with programming. I was excited to show what I could do in code if I was able to write,
so I worked tirelessly to create the game I was imagining.&lt;/p&gt;
&lt;p&gt;I wanted to combine the concepts of the Undyne and Muffet fights into one. Instead of three tracks like Muffet,
I made it a 5x5 grid where the player jumps between cells. I then took the idea of the Undyne fight, and made some bullets
able to be blocked by a shield that rotated around the player. Additionally, some bullets were not able
to be blocked so the player needed to both move around and block to survive. The game is still playable on Scratch as
&amp;ldquo;&lt;a href=&#34;https://scratch.mit.edu/projects/214045899/&#34;&gt;5x5 Bullet Avoid Game&lt;/a&gt;&amp;rdquo; by WillPar. I definitely over-achieved in the
assignment, and I think I got a 98% if I recall correctly. The teacher took off 2 points because I did not specify
in the instructions that by WASD I was referring to the keys on the keyboard to move the character (she was an art teacher
and not well-versed in technology).&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/pond_jumper/scratch_game.png&#34; alt=&#34;My first game on Scratch.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/pond_jumper/scratch_game.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;My first game on Scratch.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;h2 id=&#34;pond-jumper&#34;&gt;Pond Jumper&lt;/h2&gt;
&lt;p&gt;Back to the start of my senior year in high school and I knew I wanted to make a game in Unity. I understood that C# was close
to Java, and I knew Java well from taking AP Computer Science A the previous year. I had a few ideas but none of them stuck as well
as taking another shot at my bullet-hell idea. This time I wanted to make it 3D, with better graphics, and playable on mobile with swiping
as the main input method. I would call the app &amp;ldquo;Pond Jumper&amp;rdquo;, and theme the game around a character with a shield blocking birds while
jumping between lily pads on a pond.&lt;/p&gt;
&lt;p&gt;This was my first time writing code other than in Java for a class, and I was surprised about how easily the language concepts from Java were
able to transfer to this project. The biggest change was that I was using a framework for the first time, and while I didn&amp;rsquo;t completely
understand some of the parts of the framework and all of the new options presented to me, I found that creating a game was probably the
best way for me to be introduced to the concept of a framework. Coding tutorials for games are some of the most common types of coding
tutorials, and extremely high-quality sources exist like &lt;a href=&#34;https://www.youtube.com/channel/UCYbK_tjZ2OrIZFBvU6CCMiA&#34;&gt;Brackeys&lt;/a&gt; on YouTube.
These early lessons about working with the framework I found to not really be taught in college, but a critical part of knowing how to build
any sort of real application.&lt;/p&gt;
&lt;p&gt;I decided to use prebuilt assets from the Unity Asset Store. I had made clear that I was building the app as an exercise in programming,
and not for the artistic side, so this was fine to do. There was also no sound, but the prebuilt assets did have animations for when
the player died at least.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;display: flex; justify-content: center; align-items: center;&#34;&gt;
&lt;figure  itemprop=&#34;associatedMedia&#34;
  itemscope itemtype=&#34;http://schema.org/ImageObject&#34; &gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;http://localhost:1313/past-projects/pond_jumper/pond_jumper_game.png&#34; alt=&#34;A screenshot of Pond Jumper mid-game.&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;http://localhost:1313/past-projects/pond_jumper/pond_jumper_game.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;
          &lt;p&gt;A screenshot of Pond Jumper mid-game.&lt;/p&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The programming for the game honestly was not bad in comparison to all of the work I needed to do for AP Computer Science A, and despite
having an incredible senior project for the school it felt like the coding was easy for what I was doing. It heavily relied on
timers to know when to spawn bullets, or now birds to match the theme with my assets, and to properly give points to the player. I needed to
keep track of the birds despawning once they were out of range of the camera, and I made a predefined jump script for the character so that it
always looked like it was on a grid tile, or now lily pad for movement. I learned about game balancing, making sure that the game became more
difficult as the game went on so that it would not be endlessly boring, and added additional assets to the game so it was interesting to look at.&lt;/p&gt;
&lt;p&gt;The final component of the project was releasing the app to the Google Play Store. I chose to only release to the Play Store,
as I had an Android phone, and that Google has a one-time cost to publish compared to the yearly cost on iOS. The publishing process was
much easier back then than it is today. Google now requires 20 beta testers before launch, but that requirement did not exist
at all in 2020. Additionally, Unity had clear instructions on how to build the game to be published to the Play Store, and I learned how to
publish to the platform.&lt;/p&gt;
&lt;p&gt;Over time, I forgot to check the email associated with the project. Unity deleted the cloud backup of the project
and the Play Store took down the game due to inactivity. While there is not much of a record of the game anymore,
the app can still be downloaded from mirror sites like on APK Pure from the link below:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://apkpure.com/pond-jumper/com.WilliamParker.PondJumper&#34;&gt;https://apkpure.com/pond-jumper/com.WilliamParker.PondJumper&lt;/a&gt;&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Moore&#39;s Falls Historical Plaques Eagle Scout Project</title>
      <link>http://localhost:1313/post/community-work/moores-falls/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/community-work/moores-falls/</guid>
      
        <description>&lt;p&gt;Every town has their stories of how they were created and developed over the years.
In my hometown of Litchfield, New Hampshire, I had the opportunity to re-discover that history and
create historical plaques about it for visitors to see.&lt;/p&gt;
&lt;p&gt;The project consisted of five pages, or plaques, spread through the trail loop slowly explaining the
historical significance of the &lt;a href=&#34;https://www.google.com/maps/place/Moore%27s&amp;#43;Falls&amp;#43;Conservation&amp;#43;Area/@42.889804,-71.4576986,802m/data=!3m2!1e3!4b1!4m6!3m5!1s0x89e24cbf7350056d:0xed373f3adc07f897!8m2!3d42.8898001!4d-71.4551237!16s%2Fg%2F11c528d249?entry=ttu&amp;amp;g_ep=EgoyMDI1MDIwNC4wIKXMDSoASAFQAw%3D%3D&#34;&gt;Moore&amp;rsquo;s Falls Trail&lt;/a&gt;
over the course of the hike.&lt;/p&gt;
&lt;p&gt;This project consisted of several factors. Funding for the project needed to be secured via work with local businesses and the local newspaper.
Stakeholders needed updates on the project and proof of work to approve the necessary steps. The most notable stakeholder was
&lt;a href=&#34;https://www.facebook.com/litchfieldnhconservationcommission/&#34;&gt;The Litchfield Conservation Commission&lt;/a&gt;, a governmental board of the town
responsible for the trails and the group that originally requested the project. Finally, it required organizing and leading a team of
scouts in building and setting the plaques across the trail.&lt;/p&gt;
&lt;p&gt;In retrospect, the original writings were a bit long considering the length of the trail, but it was a culmination of
research between multiple town historians, including Chuck Mower, The book
&lt;a href=&#34;https://www.google.com/books/edition/The_Incredible_Ditch/DmGCsEpGExQC&#34;&gt;The Incredible Ditch: A Bicentennial History of the Middlesex Canal&lt;/a&gt;,
and most notably &lt;a href=&#34;http://www.middlesexcanal.org/&#34;&gt;The Middlesex Canal Association&lt;/a&gt;,
a historical group and museum dedicated to preserving the history of the canal system on the Merrimack River.&lt;/p&gt;
&lt;p&gt;Additionally, I would like to thank &lt;a href=&#34;https://lccpnh.org/scouts-bsa/&#34;&gt;BSA Troop 11&lt;/a&gt;, as the project would not have been possible without them.&lt;/p&gt;



&lt;object
    data=&#34;/community-work/moores-falls/moores_falls_combined.pdf&#34; width=&#34;100%&#34; height=&#34;800px&#34;type=&#34;application/pdf&#34;
&gt;
    &lt;p&gt;Unable to display PDF file. &lt;a href=&#34;http://localhost:1313/community-work/moores-falls/moores_falls_combined.pdf&#34;&gt;Download&lt;/a&gt; instead.&lt;/p&gt;
&lt;/object&gt;
</description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/post/current-projects/thesis-selective-pose-detection/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/current-projects/thesis-selective-pose-detection/</guid>
      
        <description></description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/post/education/snhu-comp-sci/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/education/snhu-comp-sci/</guid>
      
        <description></description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/post/mobile-projects/yabi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/mobile-projects/yabi/</guid>
      
        <description></description>
      
    </item>
    
    <item>
      <title></title>
      <link>http://localhost:1313/post/web-projects/roundhouse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <author>wiparker2020@gmail.com (William Parker)</author>
      <guid>http://localhost:1313/post/web-projects/roundhouse/</guid>
      
        <description></description>
      
    </item>
    
  </channel>
</rss>
